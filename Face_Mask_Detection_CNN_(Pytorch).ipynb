{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qGA_93HSud5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7b04cd-ab1f-4e1f-d1d3-c9ac53d85225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install xmltodict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FWyQk3Muvqq"
      },
      "outputs": [],
      "source": [
        "#importing required libraries\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import  xml.dom.minidom as minidom\n",
        "import xmltodict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "from glob import glob\n",
        "from pathlib import Path, PurePath\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0ipA9vUuvvE"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWcW2zWduvyN"
      },
      "outputs": [],
      "source": [
        "ANNOTATIONS = \"/content/drive/MyDrive/Face Mask Data/annotations\"\n",
        "IMAGES = \"/content/drive/MyDrive/Face Mask Data/images\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YHz_hCmyWlO"
      },
      "outputs": [],
      "source": [
        "#no. of images\n",
        "images = glob(IMAGES + \"/*\")\n",
        "images = sorted(images)\n",
        "len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxsRAvS2yeoE"
      },
      "outputs": [],
      "source": [
        "# no of annotations\n",
        "annotations = glob(f\"{ANNOTATIONS}/*\")\n",
        "annotations = sorted(annotations)\n",
        "len(annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgBnomjbzR18"
      },
      "outputs": [],
      "source": [
        "images[0], annotations[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EDA**"
      ],
      "metadata": {
        "id": "Qr8LJYRAB22e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU5_V7dY0iz8"
      },
      "outputs": [],
      "source": [
        "first_nine_images_shape = []\n",
        "rows, cols = 3, 3\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "for ind, img in enumerate(images):\n",
        "    if ind==9:\n",
        "        break\n",
        "    im = Image.open(img)\n",
        "    first_nine_images_shape.append(im.size)\n",
        "    plt.subplot(rows, cols, ind+1)\n",
        "    plt.imshow(im)\n",
        "    im.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfsAG9W80m_a"
      },
      "outputs": [],
      "source": [
        "first_nine_images_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Resizing images to the same dimensions***"
      ],
      "metadata": {
        "id": "zGsGunUwVLzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXzGWpZJ1KES"
      },
      "outputs": [],
      "source": [
        "SIZE = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6vKuKnV1c15"
      },
      "outputs": [],
      "source": [
        "def visualize_xml(annotations, images):\n",
        "    for i in range(len(images)):\n",
        "        img_file = images[i]\n",
        "        xml_file = annotations[i]\n",
        "        img = cv2.imread(img_file)\n",
        "        if img is not None:\n",
        "            dom = minidom.parse(xml_file)\n",
        "            root = dom.documentElement\n",
        "            objects = dom.getElementsByTagName(\"object\")\n",
        "            print(objects)\n",
        "            j = 0\n",
        "            for object in objects:\n",
        "                bndbox = root.getElementsByTagName('bndbox')[j]\n",
        "                xmin = bndbox.getElementsByTagName('xmin')[0]\n",
        "                ymin = bndbox.getElementsByTagName('ymin')[0]\n",
        "                xmax = bndbox.getElementsByTagName('xmax')[0]\n",
        "                ymax = bndbox.getElementsByTagName('ymax')[0]\n",
        "\n",
        "                xmin_data = xmin.childNodes[0].data\n",
        "                ymin_data = ymin.childNodes[0].data\n",
        "                xmax_data = xmax.childNodes[0].data\n",
        "                ymax_data = ymax.childNodes[0].data\n",
        "\n",
        "                print(object)\n",
        "                print(xmin_data)\n",
        "                print(ymin_data)\n",
        "\n",
        "                j += 1\n",
        "\n",
        "                cv2.rectangle(img,\n",
        "                              (int(xmin_data), int(ymin_data)),\n",
        "                              (int(xmax_data), int(ymax_data)),\n",
        "                              (55, 255, 155),\n",
        "                              2\n",
        "                             )\n",
        "            cv2.imwrite('/content/drive/MyDrive/Face Mask Data/images/maksssksksss0.png', img)\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cl-iz782JjJ"
      },
      "outputs": [],
      "source": [
        "for i in range(len(images)):\n",
        "    img_file = images[i]\n",
        "    xml_file = annotations[i]\n",
        "    img = cv2.imread(img_file)\n",
        "    if img is not None:\n",
        "        dom = minidom.parse(xml_file)\n",
        "        root = dom.documentElement\n",
        "        objects = dom.getElementsByTagName(\"object\")\n",
        "        j = 0\n",
        "        for obj in objects:\n",
        "            bndbox = root.getElementsByTagName('bndbox')[j]\n",
        "            xmin = bndbox.getElementsByTagName('xmin')[0]\n",
        "            ymin = bndbox.getElementsByTagName('ymin')[0]\n",
        "            xmax = bndbox.getElementsByTagName('xmax')[0]\n",
        "            ymax = bndbox.getElementsByTagName('ymax')[0]\n",
        "\n",
        "            xmin_data = xmin.childNodes[0].data\n",
        "            ymin_data = ymin.childNodes[0].data\n",
        "            xmax_data = xmax.childNodes[0].data\n",
        "            ymax_data = ymax.childNodes[0].data\n",
        "\n",
        "            print(obj)\n",
        "            print(xmin_data)\n",
        "            print(ymin_data)\n",
        "\n",
        "            j += 1\n",
        "\n",
        "            cv2.rectangle(img,\n",
        "                          (int(xmin_data), int(ymin_data)),\n",
        "                          (int(xmax_data), int(ymax_data)),\n",
        "                          (55, 255, 155),\n",
        "                          2\n",
        "                         )\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvxdijam2Oxw"
      },
      "outputs": [],
      "source": [
        "img = Image.open('/content/drive/MyDrive/Face Mask Data/images/maksssksksss0.png')\n",
        "plt.imshow(img)\n",
        "img.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcM6xwoB5YAJ"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Face Mask Data/annotations/maksssksksss0.xml\") as fd:\n",
        "    doc=xmltodict.parse(fd.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewvo8VFq5YC6"
      },
      "outputs": [],
      "source": [
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC81PbWc5YFm"
      },
      "outputs": [],
      "source": [
        "img1 = cv2.imread(\"/content/drive/MyDrive/Face Mask Data/images/maksssksksss0.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NljzNYP5YIO"
      },
      "outputs": [],
      "source": [
        "doc['annotation']['object'][0]['bndbox'].values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5lsqaTo5YK3"
      },
      "outputs": [],
      "source": [
        "coords = []\n",
        "labels = []\n",
        "for i in doc['annotation']['object']:\n",
        "    bbox = []\n",
        "    coords.append([int(i['bndbox']['xmin']),\n",
        "                   int(i['bndbox']['xmax']),\n",
        "                   int(i['bndbox']['ymin']),\n",
        "                   int(i['bndbox']['ymax'])])\n",
        "    labels.append(i['name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_VV7d_G5YNh"
      },
      "outputs": [],
      "source": [
        "coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg18I1dr5YP8"
      },
      "outputs": [],
      "source": [
        "img3 = cv2.rectangle(img1,\n",
        "              (coords[0][0], coords[0][1]),\n",
        "              (coords[0][2], coords[0][3]),\n",
        "              (255, 0, 0),\n",
        "              2\n",
        "             )\n",
        "plt.title(\"Object Representation\")\n",
        "plt.imshow(img3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioXuyrVL5YSz"
      },
      "outputs": [],
      "source": [
        "def plot_coords(coords, color, text, ax):\n",
        "    x, y, w, h = list(map(int, coords.values()))\n",
        "    mpatch = mpatches.Rectangle(\n",
        "        (x ,y),\n",
        "        w-x, h-y,\n",
        "        linewidth=1, edgecolor=color, facecolor=\"none\"\n",
        "    )\n",
        "    ax.add_patch(mpatch)\n",
        "    rx, ry = mpatch.get_xy()\n",
        "    ax.annotate(text, (rx, ry), color=color, weight='bold', fontsize=10, ha='left', va='baseline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQBTuNj35YVC"
      },
      "outputs": [],
      "source": [
        "def annotate_image(img, xml_file):\n",
        "    with open(xml_file) as f:\n",
        "        doc = xmltodict.parse(f.read())\n",
        "\n",
        "    image = plt.imread(img)\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.axis(\"off\")\n",
        "    fig.set_size_inches(10, 5)\n",
        "\n",
        "    temp = doc['annotation']['object']\n",
        "\n",
        "    if isinstance(temp, list):\n",
        "        for i in range(len(temp)):\n",
        "            if temp[i][\"name\"] == \"with_mask\":\n",
        "                plot_coords(temp[i]['bndbox'], 'green', temp[i][\"name\"],ax)\n",
        "\n",
        "            if temp[i]['name'] == \"mask_weared_incorrect\":\n",
        "                plot_coords(temp[i]['bndbox'], 'yellow', temp[i][\"name\"], ax)\n",
        "\n",
        "            if temp[i]['name'] == \"without_mask\":\n",
        "                plot_coords(temp[i]['bndbox'], 'red', temp[i][\"name\"], ax)\n",
        "    else:\n",
        "        x, y, w, h = list(map(int, temp[\"bndbox\"].values()))\n",
        "        edgecolor = {\"with_mask\": \"g\", \"without_mask\": \"r\", \"mask_weared_incorrect\": \"y\"}\n",
        "        mpatch = mpatches.Rectangle(\n",
        "            (x, y),\n",
        "            w-x, h-y,\n",
        "            linewidth=1,\n",
        "            edgecolor=edgecolor[temp[\"name\"]],\n",
        "            facecolor=\"none\")\n",
        "        ax.add_patch(mpatch)\n",
        "        rx, ry = mpatch.get_xy()\n",
        "        ax.annotate(temp[\"name\"], (rx, ry),\n",
        "                    color=edgecolor[temp[\"name\"]], weight='bold',\n",
        "                    fontsize=10, ha='left', va='baseline')\n",
        "    ax.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-5ZSsJs5YYF"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 8):\n",
        "    annotate_image(images[i], annotations[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y6W8Npd5Yaw"
      },
      "outputs": [],
      "source": [
        "immg = annotate_image(\"/content/drive/MyDrive/Face Mask Data/images/maksssksksss10.png\",\n",
        "              \"//content/drive/MyDrive/Face Mask Data/annotations/maksssksksss10.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F08wa3vE5YdN"
      },
      "outputs": [],
      "source": [
        "annotated_dataset_folder = \"/content/drive/MyDrive/Face Mask Data/annotated_dataset\"\n",
        "os.mkdir(annotated_dataset_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ReS4BFs5YgL"
      },
      "outputs": [],
      "source": [
        "{'annotation': {'folder': 'images',\n",
        "  'filename': 'maksssksksss0.png',\n",
        "  'size': {'width': '512', 'height': '366', 'depth': '3'},\n",
        "  'segmented': '0',\n",
        "  'object': [{'name': 'without_mask',\n",
        "    'pose': 'Unspecified',\n",
        "    'truncated': '0',\n",
        "    'occluded': '0',\n",
        "    'difficult': '0',\n",
        "    'bndbox': {'xmin': '79', 'ymin': '105', 'xmax': '109', 'ymax': '142'}},\n",
        "   {'name': 'with_mask',\n",
        "    'pose': 'Unspecified',\n",
        "    'truncated': '0',\n",
        "    'occluded': '0',\n",
        "    'difficult': '0',\n",
        "    'bndbox': {'xmin': '185', 'ymin': '100', 'xmax': '226', 'ymax': '144'}},\n",
        "   {'name': 'without_mask',\n",
        "    'pose': 'Unspecified',\n",
        "    'truncated': '0',\n",
        "    'occluded': '0',\n",
        "    'difficult': '0',\n",
        "    'bndbox': {'xmin': '325', 'ymin': '90', 'xmax': '360', 'ymax': '141'}}]}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL0mIrsJ5Yil"
      },
      "outputs": [],
      "source": [
        "classes = ['with_mask', 'without_mask', 'mask_weared_incorrect']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC3-BJfZ5Yla"
      },
      "outputs": [],
      "source": [
        "for i in classes:\n",
        "    os.mkdir(os.path.join(annotated_dataset_folder, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtwAtZQ_5Yqc"
      },
      "outputs": [],
      "source": [
        "def plot_images(img, vertices, image_path, width, height):\n",
        "    imgg = cv2.imread(img)\n",
        "    imgg = cv2.resize(imgg, (width, height), interpolation=cv2.INTER_AREA)\n",
        "    for i in range(len(vertices)):\n",
        "        imgg = cv2.rectangle(imgg,\n",
        "                             (int(vertices[0]), int(vertices[1])),\n",
        "                             (int(vertices[2]), int(vertices[3])),\n",
        "                             (255, 0, 0),\n",
        "                             2)\n",
        "    cv2.imwrite(image_path, imgg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5Qp6hiU5Ys-"
      },
      "outputs": [],
      "source": [
        "class_dict = {\"with_mask\": 0, \"without_mask\": 1, \"mask_weared_incorrect\": 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG7ugnyj5Yvx"
      },
      "outputs": [],
      "source": [
        "def create_df_uniclass(xmls, new_height, new_width, dataset_folderannotated_dataset_folder):\n",
        "\n",
        "    df = {\n",
        "#         \"image_array\": [],\n",
        "        \"image_path\": [],\n",
        "        \"new_image_path\": [],\n",
        "        \"og_width\": [],\n",
        "        \"og_height\": [],\n",
        "        \"new_width\": [],\n",
        "        \"new_height\": [],\n",
        "        \"xmin\": [], # [ [ [x1,y1,x2,y2], [x1,x2,y1,y2] ], [ [], [] ], [ [] ] ]\n",
        "        \"xmax\": [],\n",
        "        \"ymin\": [],\n",
        "        \"ymax\": [],\n",
        "        \"classes\": [],\n",
        "        \"vertices\": [],\n",
        "        \"labels\": [] # [ [1, 2, 3], [] ]\n",
        "    }\n",
        "    for xml_file in xmls:\n",
        "        with open(xml_file) as f:\n",
        "            doc = xmltodict.parse(f.read())\n",
        "\n",
        "        img_name = \"/content/drive/MyDrive/Face Mask Data/images/\" + doc['annotation']['filename']\n",
        "        width = int(doc['annotation']['size']['width'])\n",
        "        height = int(doc['annotation']['size']['height'])\n",
        "        temp = doc['annotation']['object']\n",
        "\n",
        "\n",
        "        if isinstance(temp, list):\n",
        "\n",
        "            for i in range(len(temp)):\n",
        "                df['image_path'].append(img_name)\n",
        "                df['og_width'].append(width)\n",
        "                df['og_height'].append(height)\n",
        "                df['new_width'].append(new_width)\n",
        "                df['new_height'].append(new_height)\n",
        "\n",
        "                x1, y1, x2, y2 = list(map(int, temp[i]['bndbox'].values()))\n",
        "                nv_x1, nv_y1, nv_x2, nv_y2 = x1 / width, y1 / height, x2 / width, y2 / height\n",
        "                n_x1, n_y1, n_x2, n_y2 = int(nv_x1 * new_width), int(nv_y1 * new_height), int(nv_x2 * new_width), int(nv_y2 * new_height)\n",
        "                fn_x1, fn_y1, fn_x2, fn_y2 = nv_x1 * new_width, nv_y1 * new_height, nv_x2 * new_width, nv_y2 * new_height\n",
        "\n",
        "                dataset_folder = os.path.join(annotated_dataset_folder, temp[i][\"name\"])\n",
        "                new_image_path = os.path.join(dataset_folder, Path(img_name).name)\n",
        "                df['new_image_path'].append(new_image_path)\n",
        "\n",
        "\n",
        "                df['labels'].append(temp[i][\"name\"])\n",
        "                df['xmax'].append(n_x2)\n",
        "                df['xmin'].append(n_x1)\n",
        "                df['ymax'].append(n_y2)\n",
        "                df['ymin'].append(n_y1)\n",
        "                df['classes'].append(class_dict[temp[i][\"name\"]])\n",
        "                df['vertices'].append([fn_x1, fn_y1, fn_x2, fn_y2])\n",
        "                plot_images(img_name, [n_x1, n_y1, n_x2, n_y2], new_image_path, new_width, new_height)\n",
        "\n",
        "        else:\n",
        "            df['image_path'].append(img_name)\n",
        "            df['og_width'].append(width)\n",
        "            df['og_height'].append(height)\n",
        "            df['new_width'].append(new_width)\n",
        "            df['new_height'].append(new_height)\n",
        "            x1, y1, x2, y2 = list(map(int, temp['bndbox'].values()))\n",
        "            nv_x1, nv_y1, nv_x2, nv_y2 = x1 / width, y1 / height, x2 / width, y2 / height\n",
        "            n_x1, n_y1, n_x2, n_y2 = int(nv_x1 * new_width), int(nv_y1 * new_height), int(nv_x2 * new_width), int(nv_y2 * new_height)\n",
        "            fn_x1, fn_y1, fn_x2, fn_y2 = nv_x1 * new_width, nv_y1 * new_height, nv_x2 * new_width, nv_y2 * new_height\n",
        "            dataset_folder = os.path.join(annotated_dataset_folder, temp[\"name\"])\n",
        "            new_image_path = os.path.join(dataset_folder, Path(img_name).name)\n",
        "            df['new_image_path'].append(new_image_path)\n",
        "\n",
        "\n",
        "            df['labels'].append(temp[\"name\"])\n",
        "            df['xmax'].append(n_x2)\n",
        "            df['xmin'].append(n_x1)\n",
        "            df['ymax'].append(n_y2)\n",
        "            df['ymin'].append(n_y1)\n",
        "            df['classes'].append(class_dict[temp[\"name\"]])\n",
        "            df['vertices'].append([fn_x1, fn_y1, fn_x2, fn_y2])\n",
        "            plot_images(img_name, [n_x1, n_y1, n_x2, n_y2], new_image_path, new_width, new_height)\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbFKo8j35YyS"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "df1 = create_df_uniclass(annotations, IMG_HEIGHT, IMG_WIDTH, annotated_dataset_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ChSE1kH5Y0y"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IIzLzwg5Y3J"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWtNZ5ho5Y5y"
      },
      "outputs": [],
      "source": [
        "classes = list((df1['labels'].value_counts()).index)\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vON0Yx5Y8k"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df1, stratify=df1['classes'], test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc86Of6s5Y-1"
      },
      "outputs": [],
      "source": [
        "train_images = train_df['new_image_path'].values\n",
        "test_images = test_df['new_image_path'].values\n",
        "\n",
        "train_labels = torch.from_numpy(train_df['classes'].values)\n",
        "test_labels = torch.from_numpy(test_df['classes'].values)\n",
        "\n",
        "train_bbox = torch.from_numpy(train_df[['xmin', 'xmax', 'ymin', 'ymax']].values)\n",
        "test_bbox = torch.from_numpy(test_df[['xmin', 'xmax', 'ymin', 'ymax']].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "RKrArOMV_mxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDFliJDK5ZBr"
      },
      "outputs": [],
      "source": [
        "annotated_dataset_folder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "0XgpvMyc_hFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.ImageFolder(annotated_dataset_folder,\n",
        "                   transform = transforms.Compose([\n",
        "                       transforms.Resize(IMG_WIDTH),\n",
        "                       transforms.RandomCrop(IMG_WIDTH),\n",
        "                       transforms.ToTensor()\n",
        "                   ]))\n",
        "train_data_loader = torch.utils.data.DataLoader(train, 32, shuffle=True, num_workers=3, pin_memory=True)"
      ],
      "metadata": {
        "id": "p-wJNE9v_hIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_std(data_loader):\n",
        "    sum_, squared_sum, batches = 0, 0, 0\n",
        "    for data, _ in data_loader:\n",
        "        sum_ += torch.mean(data, dim=([0, 2, 3]))\n",
        "        squared_sum += torch.mean(data ** 2, dim=([0, 2, 3]))\n",
        "        batches += 1\n",
        "\n",
        "    mean = sum_ / batches\n",
        "    std = (squared_sum / batches - mean ** 2) ** 0.5\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "24bYzCmD_5QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = get_mean_std(train_data_loader)\n",
        "mean, std"
      ],
      "metadata": {
        "id": "_M6Pw26M_5Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(IMG_HEIGHT),\n",
        "    transforms.RandomCrop(IMG_WIDTH),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ],
      "metadata": {
        "id": "rTvA8CKJ_hLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(IMG_HEIGHT),\n",
        "    transforms.CenterCrop(IMG_HEIGHT),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)])"
      ],
      "metadata": {
        "id": "WoQESpKQ_hNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "\n",
        "    def __init__(self, tensors, transforms=None):\n",
        "        self.tensors = tensors\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "#         image = self.tensors[0][index]\n",
        "        label = self.tensors[1][index]\n",
        "        bbox = self.tensors[2][index]\n",
        "\n",
        "        image = cv2.imread(self.tensors[0][index])\n",
        "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)\n",
        "#         image = image.permute(2, 0, 1)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return (image, label, bbox)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].shape[0]"
      ],
      "metadata": {
        "id": "CVg2tFVV_hQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = CustomTensorDataset((train_images, train_labels, train_bbox),transforms=train_transform)\n",
        "testset = CustomTensorDataset((test_images, test_labels, test_bbox),transforms=test_transform)"
      ],
      "metadata": {
        "id": "3AOVQRt1_hSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(trainset, BATCH_SIZE, shuffle=True)\n",
        "test_data_loader = torch.utils.data.DataLoader(testset, BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "K_jaBW88_hax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "QXlSgIDKBKCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectDetection(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels_cnn=3, bboxes=4):\n",
        "        super().__init__()\n",
        "        hidden_channel1 = 32\n",
        "        hidden_channel2 = 64\n",
        "        hidden_channel3 = 128\n",
        "        kernel_size = 3\n",
        "        stride = 1\n",
        "        padding = 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
        "                               out_channels=hidden_channel1,\n",
        "                               kernel_size=kernel_size,\n",
        "                               stride=stride,\n",
        "                               padding=padding)\n",
        "        self.conv2 = nn.Conv2d(in_channels=hidden_channel1,\n",
        "                               out_channels=hidden_channel2,\n",
        "                               kernel_size=kernel_size,\n",
        "                               stride=stride,\n",
        "                               padding=padding)\n",
        "        self.conv3 = nn.Conv2d(in_channels=hidden_channel2,\n",
        "                               out_channels=hidden_channel2,\n",
        "                               kernel_size=kernel_size,\n",
        "                               stride=stride,\n",
        "                               padding=padding)\n",
        "        self.conv4 = nn.Conv2d(in_channels=hidden_channel2,\n",
        "                               out_channels=hidden_channel3,\n",
        "                               kernel_size=kernel_size,\n",
        "                               stride=stride,\n",
        "                               padding=padding)\n",
        "        self.conv5 = nn.Conv2d(in_channels=hidden_channel3,\n",
        "                               out_channels=hidden_channel3,\n",
        "                               kernel_size=kernel_size,\n",
        "                               stride=stride,\n",
        "                               padding=padding)\n",
        "\n",
        "        self.batchnorm1 = nn.BatchNorm2d(hidden_channel1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(hidden_channel2)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(hidden_channel3)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Flatten()\n",
        "        self.cnn_layer = nn.Linear(7*7*128, out_channels_cnn)\n",
        "        self.regressor = nn.Linear(7*7*128, bboxes)\n",
        "\n",
        "    def cnn_layers(self, x):\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        return x\n",
        "\n",
        "    def feature_extractor(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.cnn_layers(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        classifier_op = self.cnn_layer(x)\n",
        "        regressor_op = self.regressor(x)\n",
        "        return (regressor_op, classifier_op)"
      ],
      "metadata": {
        "id": "XUXyI0Ox_heK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ObjectDetection(in_channels=3, out_channels_cnn=3, bboxes=4)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "CGBcR_AA_hg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "B6MIrRZW_hj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_loss_fn = nn.CrossEntropyLoss()\n",
        "bbox_loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "JIjDip_K_hmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 0.001\n",
        "opt = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "cjtS_wxS_hpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "train_loss = []\n",
        "train_accuracy = []\n",
        "test_loss = []\n",
        "test_accuracy = []"
      ],
      "metadata": {
        "id": "CZPtI7du_hrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    correct = 0\n",
        "    iterations = 0\n",
        "    iter_loss = 0\n",
        "    model.train()\n",
        "    for i, (images, labels, bbox) in enumerate(train_data_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        bbox = bbox.to(device)\n",
        "        regressor, classifier = model(images)\n",
        "\n",
        "        _, predicted = torch.max(classifier, 1)\n",
        "        predicted_bbox = bbox + regressor\n",
        "\n",
        "        clf_loss = classification_loss_fn(classifier, labels)\n",
        "        reg_loss = bbox_loss_fn(predicted_bbox, bbox)\n",
        "\n",
        "        total_loss = (clf_loss + reg_loss).clone().detach().requires_grad_(True)\n",
        "        opt.zero_grad()\n",
        "        total_loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        iter_loss += total_loss.item()\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        iterations += 1\n",
        "\n",
        "    train_loss.append(iter_loss / iterations)\n",
        "    train_accuracy.append((100 * correct / len(trainset)))\n",
        "    print(f\"Epoch [{epoch + 1} / {EPOCHS}], Training Loss: {train_loss[-1]:.3f}, Training Accuracy: {train_accuracy[-1]:.3f}\")"
      ],
      "metadata": {
        "id": "BF66OPgS_huc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss"
      ],
      "metadata": {
        "id": "Z9kyfVrX_hxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8JkVyil_hz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a65BTUmu_h2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bLXVqQ2t_h5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yhYmnSPj_h8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdA4EZ-S_h-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEv_6FPn_iBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zEP1PBuS_iEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIjQ3AIC_iGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qeFc5QD-_iI6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}